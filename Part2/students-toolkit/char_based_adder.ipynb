{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char-based Adder\n",
    "\n",
    "The focus of this part of the project is on the **Transformer neural network architecture**, which was introduced in the (now famous) paper *Attention is All you Need* by [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762). This neural network architecture is THE foundation of all modern large language models (e.g. OpenAI's GPT line of models, Anthropic Claude, Google Gemini, Meta Llama, DeepSeek, etc.). You will train a transformer network on large amounts of text data. The transformer networks in this project adhere to the general structure of OpenAI's **Generative Pretrained Transformer (GPT)** models.\n",
    "\n",
    "Once you create a database of aritmetic expressions in this notebook, you will train a small transformer to add numbers (i.e. evaluate strings of arithmetic operations, such as `'1+1='`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib numpy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Generate the addition dataset\n",
    "\n",
    "In this task, you will write code to generate and preprocess a dataset composed of strings that represent arithmetic addition expressions involving up to 2 digit non-negative integer operands. For example, `'20+30=50'` and `'2+30=32'`are allowed, but not `'300+5=305'`. You will train a transformer on a large numbers of such expressions then you will prompt it to generate the answer to the right of the equals sign. For example, once trained you could prompt the transformer with `'21+23='` and it should return `'44'`.\n",
    "\n",
    "#### Data format\n",
    "\n",
    "In this project, we will be working with text data and we are implementing a **character-level model** (unlike the word-level model used in the Word Embedding project). This means that each data sample is a sequence of `T` characters (i.e. tokens), which, for example, could be a part of a sentence or the characters that make up an arithmetic expression. So all the data samples in the dataset will have shape `(N, T)`. We int-code each char/token in the dataset based on the character's position in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from addition_dataset import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify `get_char2ind_map` and `make_addition_expressions` outputs\n",
    "\n",
    "In the cell below, use the provided `get_char2ind_map` to assign the vocabulary dictionary to a variable called `char2ind_map`. \n",
    "\n",
    "To aid in interpretability of the int-coded tokens, we will represent the each `0-9` digit of the integer operands being summed as `0-9` in the int coding. We map the following chars to the next available ints in our coding scheme:\n",
    "- `'+'` → 10\n",
    "- `'='` → 11\n",
    "\n",
    "We introduce a \"fake\" token `'.'` (int code: 12) within our vocabulary, our data samples, and our labels, which indicates the end of each addition expression (**end token**). For example `'47+51=98.'`. This helps the transformer know when the last \"real\" token/char in each addition expression has been reached (i.e. there are no more numbers to the right) and when it generates text after training, the transformer can output the int code corresponding `'.'` to signify that it is done generating text.\n",
    "\n",
    "We introduce another \"fake\" token `'#'` (int code: 13), which we call the **padding token**. Our transformers must be trained on fixed-length sequences, but different addition expressions have different length (i.e. the length of `'1+1=2'` is shorter than `'1+9=10'`). To overcome this issue with samples in our dataset, we use the padding token to right-pad any expression that has fewer characters than our longest supported expression `'99+99=198'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Character to index mapping:\\n\", char2ind_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When executed, the function should print:\n",
    "\n",
    "```\n",
    "Character to index mapping:\n",
    "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '+': 10, '=': 11, '.': 12, '#': 13}\n",
    "```\n",
    "\n",
    "\n",
    "In the cell below, use the provided `make_addition_expressions` to generate 10,000 addition expressions involving at most 2 digit operands (i.e. maximum operand of `99`). Use the default random seed. Assign the list of addition expressions to a variable called `addition_ds`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When executed, the function should print:\n",
    "```\n",
    "First 5/10000 expressions:\n",
    "  ['1', '+', '6', '8', '=', '6', '9', '.', '#', '#']\n",
    "  ['5', '9', '+', '5', '=', '6', '4', '.', '#', '#']\n",
    "  ['9', '0', '+', '2', '2', '=', '1', '1', '2', '.']\n",
    "  ['2', '5', '+', '1', '8', '=', '4', '3', '.', '#']\n",
    "  ['3', '3', '+', '1', '7', '=', '5', '0', '.', '#']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create addition int-coded dataset samples and labels\n",
    "\n",
    "To do this we must\n",
    "- convert each char in each addition expression into an int-code (*using the vocabulary*).\n",
    "- define the \"class labels\" or the characters we want the transformer to predict at each time step. This is simply the int code of next character in each the expression to the right of the current one. For example, for `'9+2=11'` the first token in the data sample is `9` and the first class label is `10` (the int code for `'+'`).\n",
    "\n",
    "Implement `make_addition_samples_and_labels` in `addition_dataset.py` to perform the above tasks then test your code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_int_test, y_int_test = make_addition_samples_and_labels(addition_ds, char2ind_map)\n",
    "\n",
    "print('First few samples (encoded):')\n",
    "for i in range(5):\n",
    "    print(x_int_test[i], \"=>\", y_int_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above should print:\n",
    "\n",
    "```\n",
    "First few samples (encoded):\n",
    "[1, 10, 6, 8, 11, 6, 9, 12, 13] => [10, 6, 8, 11, 6, 9, 12, 13, 13]\n",
    "[5, 9, 10, 5, 11, 6, 4, 12, 13] => [9, 10, 5, 11, 6, 4, 12, 13, 13]\n",
    "[9, 0, 10, 2, 2, 11, 1, 1, 2] => [0, 10, 2, 2, 11, 1, 1, 2, 12]\n",
    "[2, 5, 10, 1, 8, 11, 4, 3, 12] => [5, 10, 1, 8, 11, 4, 3, 12, 13]\n",
    "[3, 3, 10, 1, 7, 11, 5, 0, 12] => [3, 10, 1, 7, 11, 5, 0, 12, 13]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary converting int-coded tokens back into chars\n",
    "\n",
    "Your transformer will output ints — the int-coded representation of the predicted next char. For example, if the transformer outputs `'11'` that should be converted to `'='` for interpretability. \n",
    "\n",
    "Implement and test `make_ind2char_mapping` in `addition_dataset.py` to create the dictionary that will use the vocabulary to map int-coded representations of tokens back to chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2char_map = make_ind2char_mapping(char2ind_map)\n",
    "\n",
    "print('Here is your ind2char_map:\\n', ind2char_map)\n",
    "print('Here is your char2ind_map:\\n', char2ind_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above should print:\n",
    "```\n",
    "Here is your ind2char_map:\n",
    " {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9', 10: '+', 11: '=', 12: '.', 13: '#'}\n",
    "Here is your char2ind_map:\n",
    " {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '+': 10, '=': 11, '.': 12, '#': 13}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert from int-coded tokens back to characters\n",
    "\n",
    "Because you will prompt your transformer with string input (like a chatbot) and we would like to make sense of the transformer predictions, let's write a function (`convert_int2str`) that automates the process of taking int-coded samples back into human-readable characters then test it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_str_test = convert_int2str(x_int_test, ind2char_map)\n",
    "y_str_test = convert_int2str(y_int_test, ind2char_map)\n",
    "\n",
    "print('First few samples converted back to chars:')\n",
    "for i in range(5):\n",
    "    print(x_str_test[i], \"=>\", y_str_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell should output:\n",
    "\n",
    "```\n",
    "First few samples converted back to chars:\n",
    "['1', '+', '6', '8', '=', '6', '9', '.', '#'] => ['+', '6', '8', '=', '6', '9', '.', '#', '#']\n",
    "['5', '9', '+', '5', '=', '6', '4', '.', '#'] => ['9', '+', '5', '=', '6', '4', '.', '#', '#']\n",
    "['9', '0', '+', '2', '2', '=', '1', '1', '2'] => ['0', '+', '2', '2', '=', '1', '1', '2', '.']\n",
    "['2', '5', '+', '1', '8', '=', '4', '3', '.'] => ['5', '+', '1', '8', '=', '4', '3', '.', '#']\n",
    "['3', '3', '+', '1', '7', '=', '5', '0', '.'] => ['3', '+', '1', '7', '=', '5', '0', '.', '#']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train-validation split\n",
    "\n",
    "Implement `make_train_val_split` in `addition_dataset.py` to divide the dataset into training and validation split then run the code below to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_test, y_train_test, x_val_test, y_val_test = make_train_val_split(x_int_test, y_int_test)\n",
    "print(f'The training samples shape is: {x_train_test.shape} and should be torch.Size([9000, 9]).')\n",
    "print(f'The training labels shape is: {y_train_test.shape} and should be torch.Size([9000, 9]).')\n",
    "print(f'The validation samples shape is: {x_val_test.shape} and should be torch.Size([1000, 9]).')\n",
    "print(f'The validation labels shape is: {y_val_test.shape} and should be torch.Size([1000, 9]).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create addition dataset prompts and expected output\n",
    "\n",
    "After training the transformer on expressions such as `'1+1=2.####'`, you will prompt it with `1+1=` and we expect it to generate `'2.'` (the `'.'` indicates it is done generating text). Let's write the `split_sum_and_answer` function to automate the process of generating the prompts and expected outputs for samples in either the train or validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lhs_lists, ans_lists = split_sum_and_answer(x_str_test)\n",
    "\n",
    "print('First five training prompts and answers:')\n",
    "for i in range(5):\n",
    "    print(f'prompt: {lhs_lists[i]} | answer: {ans_lists[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell should output:\n",
    "\n",
    "```\n",
    "First five training prompts and answers:\n",
    "prompt: 1+68= | answer: 69.#\n",
    "prompt: 59+5= | answer: 64.#\n",
    "prompt: 90+22= | answer: 112\n",
    "prompt: 25+18= | answer: 43.\n",
    "prompt: 33+17= | answer: 50.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automate addition dataset preprocessing\n",
    "\n",
    "Call the functions that you wrote to get and preprocess the addition dataset all in one function called `get_addition_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_test, y_train_test, x_val_test, y_val_test = get_addition_dataset(N=100)\n",
    "print(f'The shape of your training samples is {x_train_test.shape} and it should be torch.Size([90, 9]).')\n",
    "print(f'The shape of your training labels is {y_train_test.shape} and it should be torch.Size([90, 9]).')\n",
    "print(f'The shape of your val samples is {x_val_test.shape} and it should be torch.Size([10, 9]).')\n",
    "print(f'The shape of your val labels is {y_val_test.shape} and it should be torch.Size([10, 9]).')\n",
    "\n",
    "# We need int coded everything!\n",
    "assert x_train_test.dtype == torch.long\n",
    "assert y_train_test.dtype == torch.long\n",
    "assert x_val_test.dtype == torch.long\n",
    "assert y_val_test.dtype == torch.long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Train a GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from mingpt.model import GPT\n",
    "from mingpt.trainer import Trainer\n",
    "from mingpt.utils import set_seed, setup_logging, CfgNode as CN\n",
    "\n",
    "# Set multiprocessing start method for Jupyter compatibility\n",
    "multiprocessing.set_start_method('fork', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "\n",
    "    C = CN()\n",
    "\n",
    "    # system\n",
    "    C.system = CN()\n",
    "    C.system.seed = 3407\n",
    "    C.system.work_dir = './outputs/'\n",
    "\n",
    "    # model\n",
    "    C.model = GPT.get_default_config()\n",
    "    C.model.model_type = 'gpt-nano'\n",
    "\n",
    "    # trainer\n",
    "    C.trainer = Trainer.get_default_config()\n",
    "    C.trainer.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class is used by the Trainer to get batches of data\n",
    "class AdditionSubset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return x (input ids) and y (labels) as torch tensors\n",
    "        return self.x_data[idx], self.y_data[idx]\n",
    "\n",
    "\n",
    "class AdditionDataset(Dataset):   \n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        C = CN()\n",
    "        C.ndigit = 2\n",
    "        return C\n",
    "\n",
    "    def __init__(self):\n",
    "        self.subsets = {}\n",
    "        self.ndigit = self.get_default_config().ndigit\n",
    "\n",
    "    def add_subset(self, name, x_data, y_data):\n",
    "        self.subsets[name] = AdditionSubset(x_data, y_data)\n",
    "\n",
    "    def get_subset(self, name):\n",
    "        return self.subsets[name]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"subsets: {list(self.subsets.keys())}\"\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        # 10 digits + '+', '=', '.', '#'\n",
    "        return 14\n",
    "    \n",
    "    def get_block_size(self):\n",
    "        # render = astr + '+' + bstr + '=' + cstr + '.'  -> length = 3*ndigit + 4\n",
    "        # x will be render[:-1], so block_size = (3*ndigit + 4) - 1\n",
    "        return 3*self.ndigit + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that the dataset works as expected\n",
    "x_train, y_train, x_val, y_val = get_addition_dataset(N=100)\n",
    "\n",
    "d = AdditionSubset(x_train, y_train)\n",
    "for n, x in enumerate(d):\n",
    "    print(x)\n",
    "    if n > 4: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "setup_logging(config)\n",
    "set_seed(config.system.seed)\n",
    "\n",
    "# construct train and test datasets\n",
    "char2ind_map = get_char2ind_map()\n",
    "x_train, y_train, x_val, y_val = get_addition_dataset(N=10000)\n",
    "\n",
    "dataset = AdditionDataset()\n",
    "config.data = dataset.get_default_config()\n",
    "dataset.add_subset('train', x_train, y_train)\n",
    "dataset.add_subset('test', x_val, y_val)\n",
    "\n",
    "# construct the model\n",
    "config.model.vocab_size = dataset.get_vocab_size()\n",
    "config.model.block_size = dataset.get_block_size()\n",
    "model = GPT(config.model)\n",
    "\n",
    "# construct the trainer object\n",
    "config.trainer.max_iters = 5000\n",
    "config.trainer.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "trainer = Trainer(config.trainer, model, dataset.get_subset('train') )\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for the evaluation of a model\n",
    "def get_left_part(x, val):\n",
    "    ''' finds the position of '=' in the input sequence x and returns everything up to and including '=' '''\n",
    "    indices = (x == val).nonzero(as_tuple=True)[1]\n",
    "    return x[:, :indices[0]+1]\n",
    "\n",
    "def get_right_part(x, val):\n",
    "    indices = (x == val).nonzero(as_tuple=True)[1]\n",
    "    return x[:, indices[0]+1:]\n",
    "\n",
    "def eval_split(trainer, split, max_batches=None):\n",
    "    assert split in ['train', 'test'], \"split must be either 'train' or 'test'\"\n",
    "    subset = dataset.get_subset(split)\n",
    "    ndigit = dataset.ndigit\n",
    "    results = []\n",
    "    mistakes_printed_already = 0\n",
    "    loader = DataLoader(subset, batch_size=100, num_workers=0, drop_last=False, pin_memory=False, persistent_workers=False)\n",
    "    for b, (x, y) in enumerate(loader):\n",
    "        for i in range(x.size(0)):\n",
    "            question = get_left_part(x[i:i+1], char2ind_map['=']) # Working with 2D tensors\n",
    "            answer = get_right_part(y[i:i+1], char2ind_map['=']) # Working with 2D tensors\n",
    "            # let the model sample the rest of the sequence\n",
    "            result = model.generate(question, ndigit+2, do_sample=False) # using greedy argmax, not sampling\n",
    "            answer2 = get_right_part(result, char2ind_map['=']) # Working with 2D tensors\n",
    "            correct = (answer2[0, :ndigit+2] == answer[0, :ndigit+2]).all(dim=0).cpu()\n",
    "            results.append(int(correct))\n",
    "            if not correct and mistakes_printed_already < 5:\n",
    "                mistakes_printed_already += 1\n",
    "                print(f\"Question: {question[0]}  GPT claims {answer2[0]} but ground truth is {answer[0 ]} \")\n",
    "        if max_batches is not None and b+1 >= max_batches:\n",
    "            break\n",
    "    rt = torch.tensor(results, dtype=torch.float)\n",
    "    print(\"%s final score: %d/%d = %.2f%% correct\" % (split, rt.sum(), len(results), 100*rt.mean()))\n",
    "    return rt.sum()\n",
    "\n",
    "# iteration callback\n",
    "top_score = 0\n",
    "def batch_end_callback(trainer):\n",
    "    global top_score\n",
    "\n",
    "    if trainer.iter_num > 0 and (trainer.iter_num+1) % 100 == 0:\n",
    "        print(f\"iter {trainer.iter_num+1}: train loss {trainer.loss.item():.5f}; iter_dt {trainer.iter_dt * 1000:.2f}ms\")\n",
    "\n",
    "    if trainer.iter_num > 0 and (trainer.iter_num+1) % 1000 == 0:\n",
    "        # evaluate both the train and test score\n",
    "        train_max_batches = {1: None, 2: None, 3: 5}[config.data.ndigit] # if ndigit=2 we can afford the whole train set, ow no\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_score = eval_split(trainer, 'train', max_batches=train_max_batches)\n",
    "            test_score  = eval_split(trainer, 'test',  max_batches=None)\n",
    "            score = train_score + test_score\n",
    "            # save the model if this is the best score we've seen so far\n",
    "            if score > top_score:\n",
    "                top_score = score\n",
    "                print(f\"new top score of {score} achieved\")\n",
    "                #print(f\"saving model with new top score of {score}\")\n",
    "                #ckpt_path = os.path.join(config.system.work_dir, \"adder.model.pt\")\n",
    "                #torch.save(model.state_dict(), ckpt_path)\n",
    "        # revert model to training mode\n",
    "        model.train()\n",
    "\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the optimization\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the trained model with your prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on custom prompts\n",
    "test_prompts = [\"1+1=\", \"5+3=\", \"12+34=\", \"5+6=\", \"99+1=\", \"42+17=\", \"8+9=\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for prompt_str in test_prompts:\n",
    "        prompt_tokens = [char2ind_map[c] for c in prompt_str]\n",
    "        prompt_tensor = torch.tensor([prompt_tokens])\n",
    "        \n",
    "        result = model.generate(prompt_tensor, max_new_tokens=4, do_sample=False)\n",
    "        result_str = ''.join([ind2char_map[int(token)] for token in result[0].tolist()])\n",
    "        \n",
    "        print(f\"Prompt: {prompt_str:10} | Model output: {result_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference results\n",
    "```\n",
    "Prompt: 1+1=       | Model output: 1+1=2.##\n",
    "Prompt: 5+3=       | Model output: 5+3=8.##\n",
    "Prompt: 12+34=     | Model output: 12+34=46.#\n",
    "Prompt: 5+6=       | Model output: 5+6=11.#\n",
    "Prompt: 99+1=      | Model output: 99+1=100.\n",
    "Prompt: 42+17=     | Model output: 42+17=69.#\n",
    "Prompt: 8+9=       | Model output: 8+9=17.#\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
