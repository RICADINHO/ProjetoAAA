1.

The NewGELU class computes an approximation of the real GELU rather than the exact definition to save computation time and resources since the real GELU requires computing the cumulative distribution function of a Gaussian that is computed with error function which is more expensive compared to the tanh function that only requires basic mathematical operations.

2.

The code line transforms the query tensor from a single large embedding vector into smaller separate sections for each attention head to process the input in parallel, grouping the data into segments by head rather than word.

3.

A: The attention score division acts as a normalization factor for the Q and K dot product, if their dimension is large the dot product can grow very large.

B: If the scaling were to be removed the dot products could become extremely large which causes several problems.

C: This line applies a mask to the attention scores so that the model can't look into the future unseen tokens and know their values.

D: If the mask were omitted, the model would be able to see into the future and the model would learn from the attention mechanism since it would already know the next tokens in line and would only learn to use the tokens ahead of the current one and would fail on the testing phase since it was train with unobtainable data.

4.

A: no, the original implementation uses Post normalization and the minGPT uses Pre normalization.

B: The key difference is where the layer of normalization is placed relative to the residual connection, the attention layer and MLP layer. On the Post normalization the normalization is applied after the residual addition x= Norm(x + SubLayer(x)), and in the Pre normalization the normalization is applied before the subLayer input, inside the residual branch x= x + SubLayer(Norm(x)). Post norm can cause vanishing gradient or exploding gradients in deep networks and might need tweaks on the learning rate, the Pre norm improves stability since there is a clear path for the gradient to follow, this improves the deep networks problem.

5.

The MLP serves as the processing part of the transformer for each token while the attention layer deals with communication between tokens. The dimensiinality is expanded to 4*n_embd to project the data into a larger space. This allows the transformer to learn more complex patterns and relationships before being compressed down again for the next block.

6.

Positional embeddings are added element wise to the token embeddings at the input stage. They are necessary because the self attention mechanism needs to know the order, the embeddings add sequence information, allowing the model to understand word order and different meanings between same sets of tokens.

7.

This line isolates the top k most likely tokens to be correct by finding the k highest score in the current distribution of tokens. Then, it deletes all other tokens by setting their score to -infinity, so that when the softmax is applied they won't be chosen.


