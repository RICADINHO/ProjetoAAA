{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e46f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ricadinho/Desktop/cenas_universidade/2_ano/1_semestre/AAA/ree/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-12-04 18:07:12.003800: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-04 18:07:12.601873: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-04 18:07:14.024662: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForTokenClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Training on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8752bf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sentences: 12544 2001 2077\n",
      "Total sentences: 2001\n",
      "First 3 sentences:\n",
      "Sentence 1:\n",
      "  Words: ['From', 'the', 'AP', 'comes', 'this', 'story', ':']\n",
      "  POS tags: ['ADP', 'DET', 'PROPN', 'VERB', 'DET', 'NOUN', 'PUNCT']\n",
      "Sentence 2:\n",
      "  Words: ['President', 'Bush', 'on', 'Tuesday', 'nominated', 'two', 'individuals', 'to', 'replace', 'retiring', 'jurists', 'on', 'federal', 'courts', 'in', 'the', 'Washington', 'area', '.']\n",
      "  POS tags: ['PROPN', 'PROPN', 'ADP', 'PROPN', 'VERB', 'NUM', 'NOUN', 'PART', 'VERB', 'VERB', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADP', 'DET', 'PROPN', 'NOUN', 'PUNCT']\n",
      "Sentence 3:\n",
      "  Words: ['Bush', 'nominated', 'Jennifer', 'M.', 'Anderson', 'for', 'a', '15', '-', 'year', 'term', 'as', 'associate', 'judge', 'of', 'the', 'Superior', 'Court', 'of', 'the', 'District', 'of', 'Columbia', ',', 'replacing', 'Steffen', 'W.', 'Graae', '.']\n",
      "  POS tags: ['PROPN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'ADP', 'DET', 'NUM', 'PUNCT', 'NOUN', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'PROPN', 'ADP', 'DET', 'PROPN', 'ADP', 'PROPN', 'PUNCT', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "# --- Funções de Leitura de Dados ---\n",
    "\n",
    "def read_conllu_file(filepath):\n",
    "    \"\"\"\n",
    "    Read a CoNLL-U format file and extract words and POS tags sentence by sentence.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the CoNLL-U file\n",
    "        \n",
    "    Returns:\n",
    "        A list of dictionaries, each containing 'words' and 'pos_tags' lists for a sentence\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    current_sentence = {'words': [], 'pos_tags': []}\n",
    "    \n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as data_file:\n",
    "        for line in data_file:\n",
    "            if line.startswith(\"#\"):\n",
    "                # Skip comment lines\n",
    "                pass\n",
    "            elif line.strip() == \"\":\n",
    "                # Empty line marks end of sentence\n",
    "                if current_sentence['words']:  # Only add non-empty sentences\n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = {'words': [], 'pos_tags': []}\n",
    "            else:\n",
    "                # Parse the token line\n",
    "                fields = line.split(\"\\t\")\n",
    "                word, pos = fields[1], fields[3]\n",
    "                current_sentence['words'].append(word)\n",
    "                current_sentence['pos_tags'].append(pos)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "#load data\n",
    "TRAIN = \"./data/en_ewt-ud-train.conllu\"\n",
    "DEV = \"./data/en_ewt-ud-dev.conllu\"\n",
    "TEST = \"./data/en_ewt-ud-test.conllu\"\n",
    "\n",
    "# --- 1) Carregar Dados ---\n",
    "try:\n",
    "    train_sents = read_conllu_file(TRAIN)\n",
    "    val_sents = read_conllu_file(DEV)\n",
    "    test_sents = read_conllu_file(TEST)\n",
    "    print(\"Loaded sentences:\", len(train_sents), len(val_sents), len(test_sents))\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Erro: Ficheiro de dados não encontrado: {e.filename}. Certifique-se de que os ficheiros CoNLL-U estão em './data/'\")\n",
    "    exit()\n",
    "\n",
    "# Display preview\n",
    "print(f\"Total sentences: {len(val_sents)}\")\n",
    "print(f\"First 3 sentences:\")\n",
    "for i, sent in enumerate(val_sents[:3]):\n",
    "    print(f\"Sentence {i+1}:\")\n",
    "    print(f\"  Words: {sent['words']}\")\n",
    "    print(f\"  POS tags: {sent['pos_tags']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98585d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " distinctive tags: 18\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "MODEL_NAME = 'distilbert-base-cased' # Case sensitive is usually better for POS\n",
    "\n",
    "# --- 1. Label Mapping (Reusing from Task 1.1) ---\n",
    "# We need to ensure we have the same tag map. \n",
    "# Re-running build_vocab just to be safe and self-contained\n",
    "def get_tag_map(sentences):\n",
    "    tags = set()\n",
    "    for sent in sentences:\n",
    "        for tag in sent['pos_tags']:\n",
    "            tags.add(tag)\n",
    "    tag2id = {tag: i for i, tag in enumerate(sorted(list(tags)))}\n",
    "    id2tag = {i: tag for tag, i in tag2id.items()}\n",
    "    return tag2id, id2tag\n",
    "\n",
    "# Load data (assuming sentences are already loaded from previous task)\n",
    "# train_sentences, dev_sentences, test_sentences are available\n",
    "tag2id, id2tag = get_tag_map(train_sents)\n",
    "num_labels = len(tag2id)\n",
    "\n",
    "print(f\" distinctive tags: {num_labels}\")\n",
    "\n",
    "# --- 2. Tokenization & Alignment Function ---\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class TransformerPOSDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sentences, tag2id, tokenizer, max_len):\n",
    "        self.sentences = sentences\n",
    "        self.tag2id = tag2id\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.sentences[idx]\n",
    "        word_list = item['words']\n",
    "        label_list = item['pos_tags']\n",
    "        \n",
    "        # Tokenize the sentence\n",
    "        # is_split_into_words=True tells the tokenizer we are providing a list of words\n",
    "        encoding = self.tokenizer(\n",
    "            word_list,\n",
    "            is_split_into_words=True,\n",
    "            return_offsets_mapping=True, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Create labels aligned with tokens\n",
    "        labels = []\n",
    "        encoded_labels = []\n",
    "        \n",
    "        # The offset mapping helps us determine which original word a token belongs to\n",
    "        # It returns tuples (start, end) char indices. (0,0) usually means special token.\n",
    "        doc_encodings = encoding.encodings[0]\n",
    "        \n",
    "        # word_ids returns a list where each element indicates the index of the word \n",
    "        # in the original sentence that the token corresponds to. \n",
    "        # None indicates special tokens like [CLS] or [SEP].\n",
    "        word_ids = encoding.word_ids()\n",
    "        \n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # Special token ([CLS], [SEP], [PAD]) -> Ignore (-100)\n",
    "                encoded_labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # First subword of a new word -> Use the real label\n",
    "                tag = label_list[word_idx]\n",
    "                encoded_labels.append(self.tag2id[tag])\n",
    "            else:\n",
    "                # Subsequent subword of the same word -> Ignore (-100)\n",
    "                # Alternatively, you could propagate the label, but -100 is standard\n",
    "                encoded_labels.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "            \n",
    "        # Squeeze to remove batch dimension added by tokenizer\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(encoded_labels)\n",
    "        \n",
    "        # Remove offset_mapping as it's not needed for the model\n",
    "        if 'offset_mapping' in item:\n",
    "             del item['offset_mapping']\n",
    "             \n",
    "        return item\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = TransformerPOSDataset(train_sents, tag2id, tokenizer, MAX_LEN)\n",
    "test_dataset = TransformerPOSDataset(test_sents, tag2id, tokenizer, MAX_LEN)\n",
    "val_dataset = TransformerPOSDataset(val_sents, tag2id, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15046019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, NUM_EPOCHS=10, learning_rate=0.001):\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "        train_loss = running_train_loss / len(train_loader)\n",
    "        \n",
    "        # --- Validation Loop ---\n",
    "        model.eval() # Set the model to evaluation mode (e.g., disables dropout)\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad(): # Disable gradient calculation for validation\n",
    "            for batch in val_loader:\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "        val_loss = running_val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96aac75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_bert = DistilBertForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=num_labels\n",
    ").to(device)\n",
    "\n",
    "print(f\"Bert Model parameters: {sum(p.numel() for p in model_bert.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5aa0b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fine-tuning...\n",
      "Epoch [1/3], Train Loss: 0.1962, Val Loss: 0.1178\n",
      "Epoch [2/3], Train Loss: 0.0533, Val Loss: 0.1167\n",
      "Epoch [3/3], Train Loss: 0.0292, Val Loss: 0.1300\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Starting Fine-tuning...\")\n",
    "\n",
    "bert = train_model(model_bert,train_loader,val_loader,EPOCHS,LEARNING_RATE)\n",
    "\n",
    "training_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea043f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, id2tag):\n",
    "    model.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits # Shape: (batch, seq_len, num_labels)\n",
    "            predictions = torch.argmax(logits, dim=2)\n",
    "            \n",
    "            # Flatten to align\n",
    "            predictions = predictions.view(-1).cpu().numpy()\n",
    "            labels = labels.view(-1).cpu().numpy()\n",
    "            \n",
    "            # Filter out ignored indices (-100)\n",
    "            # This ensures we only evaluate on the first subword of real words\n",
    "            valid_indices = labels != -100\n",
    "            \n",
    "            valid_preds = predictions[valid_indices]\n",
    "            valid_targets = labels[valid_indices]\n",
    "            \n",
    "            all_labels.extend(valid_targets)\n",
    "            all_pred.extend(valid_preds)\n",
    "\n",
    "    # Convert ids back to tags\n",
    "    true_tags = [id2tag[i] for i in all_labels]\n",
    "    pred_tags = [id2tag[i] for i in all_pred]\n",
    "    \n",
    "    acc = accuracy_score(true_tags, pred_tags)\n",
    "    report = classification_report(true_tags, pred_tags, zero_division=0)\n",
    "    \n",
    "    return acc, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67531505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.7505%\n",
      "Training time: 357.92 seconds\n",
      "Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.90      0.95      0.93      1788\n",
      "         ADP       0.97      0.98      0.98      2025\n",
      "         ADV       0.96      0.95      0.95      1191\n",
      "         AUX       1.00      1.00      1.00      1543\n",
      "       CCONJ       1.00      1.00      1.00       736\n",
      "         DET       0.99      1.00      0.99      1897\n",
      "        INTJ       0.95      0.87      0.91       121\n",
      "        NOUN       0.96      0.93      0.95      4123\n",
      "         NUM       0.95      1.00      0.97       542\n",
      "        PART       1.00      0.99      1.00       649\n",
      "        PRON       0.99      0.99      0.99      2165\n",
      "       PROPN       0.90      0.91      0.91      2075\n",
      "       PUNCT       1.00      0.99      0.99      3096\n",
      "       SCONJ       0.96      0.96      0.96       384\n",
      "         SYM       0.76      0.97      0.86       113\n",
      "        VERB       0.98      0.98      0.98      2606\n",
      "           X       0.89      0.38      0.53        42\n",
      "           _       0.97      0.99      0.98       354\n",
      "\n",
      "    accuracy                           0.97     25450\n",
      "   macro avg       0.95      0.94      0.94     25450\n",
      "weighted avg       0.97      0.97      0.97     25450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc, report = evaluate_model(model_bert, test_loader, id2tag)\n",
    "\n",
    "print(f\"Accuracy: {100*acc:.4f}%\")\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "print(f\"Report: \\n{report}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
